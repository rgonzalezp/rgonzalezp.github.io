I"M<h1 id="understanding-how-people-with-visual-impairments-take-selfies-experiences-and-challenges"><a href="">Understanding How People with Visual Impairments Take Selfies: Experiences and Challenges</a></h1>

<p>Authors: <strong><em>Ricardo E. Gonzalez Penuela</em></strong>, Paul Vermette, Zihan Yan, Cheng Zhang, Keith Vertanen, Shiri Azenkot.</p>

<h2 id="short-summary-of-work">Short Summary of Work</h2>

<p>Selfies are a pervasive form of communication in social media. While there has been some work on systems that guide people with visual impairments (PVI) in taking photos, nearly all has focused on using the camera on the back of the device. We do not know  how PVI take selfies and what are the barriers they have encountered.</p>

<p>The aim of our work is to understand:</p>

<ul>
  <li>PVI selfie-taking experiences and challenges.</li>
  <li>What information do PVI need when taking selfies.</li>
  <li>What modalities do PVI prefer (e.g., tactile, verbal, or non-verbal audio) to support selfie-taking.</li>
</ul>

<p>To address this gap, we conducted interviews with 10 PVI. Our findings show that current selfie-taking applications do not provide enough assistance to meet the needs of PVI.</p>

<p>We contribute design guidelines that researchers and designers can implement for creating accessible selfie-taking applications.</p>

<h2 id="design-guidelines-for-accessible-selfie-taking-applications">Design Guidelines For Accessible Selfie-taking applications</h2>

<p>Read more down below in the published paper!</p>
<ul>
  <li>
    <p><strong>Tailor the system guidance to the goal of the user and support all success criteria:</strong> For example, a participant
mentioned that when she wanted to take a quick photo and she did not care how she looked, but other times when
she shared the photo on social media, she cared about how she looked and the photo quality. In the first
scenario the iOS camera guidance provides enough information since it helps PVI center their face and snap a quick
photo; but in the second scenario, PVI need more information about their physical appearance (e.g., their facial
expression, whether their eyes are closed or open, clothing detected), about background information (e.g., disorganized
table, floor detected) and the quality of the photo (e.g., whether the photo is blurry, whether the faces in the photo can be
seen clearly).</p>
  </li>
  <li>
    <p><strong>Guide the user through human-like conversational prompts and descriptions:</strong> PVI are accustomed to being assisted by sighted peers when they take photos. Thus, conversational prompts
and descriptions of photo content and quality would be natural and easy to follow.</p>
  </li>
  <li><strong>Communicate low-level guidance with real-time feedback using non-verbal audio cues:</strong> PVI are familiar with systems that leverage the pitch, tone, or rhythm of sounds to convey a change of state in the system or progress of the task. This, combined with human-like conversational prompts, would provide PVI with
the necessary information to make small adjustments to the camera position.</li>
  <li><strong>Include settings to filter information:</strong> provide an easy way to turn on or off information of the guidance system (e.g., turn on/off guidance about the photoâ€™s background or peopleâ€™s facial expressions). More generally,
information should be modifiable with varying degrees of specificity to adapt to the needs and preferences of the user.</li>
</ul>

<h2 id="demo">Demo</h2>

<iframe class="demo-video" width="560" height="315" src="https://www.youtube.com/embed/404" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h2 id="related-papers--publications">Related Papers &amp; Publications</h2>

<p><a href="">To appear on Assetsâ€™22, stay tuned for the paper.</a></p>

:ET