I"À<h1 id="hands-on-using-gestures-to-control-descriptions-of-a-virtual-environment-for-people-with-visual-impairments"><a href="https://drive.google.com/file/d/1xyjGv0lWaBrHa0mP0Y3oi1QJyDNfttGG/view?usp=sharing">Hands-On: Using Gestures to Control Descriptions of a Virtual Environment for People with Visual Impairments</a></h1>

<h2 id="demo">Demo</h2>

<iframe class="demo_video" width="560" height="315" src="https://www.youtube.com/embed/uTJXJC7JgeE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>Authors: <strong><em>Ricardo E. Gonzalez Penuela</em></strong>, Wren Poremba, Christina Trice, and Shiri Azenkot.</p>

<h2 id="short-summary-of-work">Short Summary of Work</h2>
<p>Virtual reality (VR) uses three main senses to relay information: sight, sound, and touch. People with visual impairments (PVI) rely primarily on sound and haptic feedback (touch) to receive information about VR environments. While researchers have explored several approaches to make navigation and perception of objects more accessible in VR, none of them offer a natural way to request descriptions of objects, nor control of the flow of auditory information.</p>

<p>To address this gap, we designed a set of hand gestures that
PVI can perform to request descriptions about the environment.</p>
<h2 id="hand-gestures">Hand Gestures</h2>

<p>We designed a VR haptic glove based on the open-source project
LucidVR [10]. These gloves are capable of tracking the user‚Äôs finger curling, detecting hand movements in VR, and providing FFB
through the user‚Äôs fingers.</p>

<p>Informed by the gloves capabilities and previous work (e.g., See-
ingVR, VizWiz, Seeing AI, OrCam) [ 1‚Äì 3, 20 ], we designed a series
of hand gestures that can be performed to trigger the descriptions
interactively. These gestures allow the user to obtain information
about the environment, and also dynamically control the speech
rate of the voice reader.</p>

<p><img class="demo-video" height="350" src="https://rgonzalezp.github.io/src/assets/img/hands-on/figuregesturev4.jpg" /></p>

<ul>
  <li>Point with one finger (e.g., index finger) towards an object
to trigger a general description of the object (e.g. You see a
cup for drinking coffee or tea. It is floating in front of you).</li>
  <li>Point with two adjacent fingers (e.g., index finger, and the
middle finger) towards an object to trigger a more detailed
description of the object (e.g. The cup is mostly gray. It has
a drawing of a Border Collie sleeping. Inside of the cup you
can see the words: ‚Äúquiet time‚Äù).</li>
  <li>After pointing towards an object, flick to the right or to the
left to trigger a description of a nearby object in the direction
that was waved (e.g., right, left).</li>
  <li>Perform a wide wave from left to right to trigger the voice
reader to read the descriptions of the objects in the view.</li>
  <li>Make a fist and move it up or down to raise or lower the
speed of the speech of verbal feedback.</li>
</ul>

<h2 id="haptics">Haptics</h2>

<p>In addition to the gestures, PVI can also perceive a simplified
version of the shape of held objects (e.g., Cylinders, spheres, rectangular geometric figures). The gloves combine the use of retractable strings and servo motors that limit the range of motion of the spool with a horn, restricting the user‚Äôs finger curling to perceive the virtual object.</p>

<p><img class="demo-video" height="350" src="https://rgonzalezp.github.io/src/assets/img/hands-on/figuretactileperceptionwoborderv4.jpg" /></p>

<h2 id="related-papers--publications">Related Papers &amp; Publications</h2>

<p><a href="">Submitted to UIST‚Äô22, stay tuned for more info.</a></p>
:ET